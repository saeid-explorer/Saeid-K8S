============================================================================================ KUBERNETES CKA Course - UDEMY_ MUMSHAD ======================================================================================================

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Requirements after installing Kubernetes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	# sudo systemctl stop ufw && sudo systemctl disable ufw
	# echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf
	# sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab && sudo swapoff -a
	# export KUBECONFIG=/etc/kubernetes/admin.conf
	- verify if kube api server is running:
		# ps aux | grep kube-apiserver	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Cluster Architecture~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Note: The purpose of Kubernetes is to host your applications in the form of containers in an automated fasion so that you can deploy as many instances of your application and easily enable communication between different services within your application.

	- Types of Nodes and their components in Kubernetes
		
		- Master Nodes: Manage, Plan, Schedule, Monitor Nodes
			
			- Etcd Cluster
				- Etcd is a database that stores information in a key-value format

			- Kube-Scheduler
				- A scheduler identifies the right node to place a container on based on the containers reousrce requirements, the worker nodes capacity or other constraints

			- Controller Manager
				- Node-Controller
					- Takes care of nodes. Responsible for onboarding new nodes to the cluster handling situations where nodes become unavailable
				- Replication-Controller
					- Ensures that the disired number of containers are running at all times in your replication group 
				
			- Kube-api server
				- It's the primary management component of kubernetes
				- It's responsible for orchestrating all operations within the cluster
				- It Exposes the Kubernetes API which is used for external users to perform management operations on the cluster 


		- Worker Nodes: Host Application as Containers
				
			- Container Runtime Engine
				- Softwares that can run containers
				- Popular ones are Docker, Containerd, Rocket
				- Container Runtime Engine must be installed on all the nodes in the cluster(including the Master nodes)

			- Kubelet
				- A kubelet is an agent that runs on each node in a cluster
				- It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required
				- Kube-api server priodically fetches status reports from the kubelet to monitor the state of nodes and containers on them

			- kube-proxy
				- Communication between worker nodes are enable by this on the worker node
				- Ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ETCD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- ETCD is a distributed reliable key-value store that is simple, secure & fast
	- Installing
		1) Download Binaries
		2) Extract
			- tar xzvf <filename.tar.gz>
		3) Run ETCD Service
			- ./etcd
	- ETCD service listens on port 2379 by default
	- The default client that comes with ETCD is ETCD control client(etcdctl)
		- You can use it to store and retrieve key-value pairs
			- e.g.:  ./etcdctl set key1 value1
			- e.g.:  ./etcdctl get key1
		- getting help
			- ./etcdctl
	- ETC includes the following data:
		- Nodes
		- PODs
		- Configs
		- Secrets
		- Accounts
		- Roles
		- Bindings
		- Others
		- and Every Change on the cluster

* Note: The practice test environments are deployed using the kubeadm tool 

	- Setup ETCD when you deploy a cluster from scratch
		- first you download the binaries of ETCD
			- e.g.:
				- wget -q --https-only "https://github.com/coreos/etcd/releases/download/v3.5.13/etcd-v3.5.13-linux-amd64.tar.gz"
		* Note: --advertise-client-url: it's the address on which ETCD listens{Server_IP:2379}

	- Setup ETC when you deoploy a cluster using kubeadm tool
		- if you setup your cluster using kubeadm, then kubeadm deploys the ETCD server for you as a POD in the kube-system namespace
		- kubectl get pods -n kube-system
		- To list all keys stored on ETCD by Kubernetes:
			- kubectl exec etcd-master -n kube-system etcdctl get / --prefix --keys-only

	- ETCD stores data in a registry and under that there are various Kubernets constructs such as nodes, pods, replicasets, etc.

	- ETCD in HA Environment
		- in a multi-master Kubernetes Environment we should make sure the different ETCD services know about each other
			- --initial-cluster option in etcd.service defines ETCDs on a cluster


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-API ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubectl command in fact reaches to kube-api server (e.g. kubectl get nodes )
	- kubeapi first authenticates the request and validates it
	- it then retrieves the data from ETCD Cluster and responds back with the requested information
	- Actions as a result of a change request(e.g. creating a POD) in the cluster
		- Authenticate User
		- Validate Request
		- Retrieve data
		- Update ETCD
		- Scheduler
		- Kubelet

	- viewing api-server(kube-api) - when you deploy cluster using kubeadm
		- kubectl get pods -n kube-system 
		OR
		- cat /etc/kubernetes/manifests/kube-apiserver.yaml

	- viewing api-server(kube-api) - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-apiserver.service
		- ps -aux | grep kube-apiserver



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube Controller Manager ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- a Controller is a process that continuously monitors the state of various components within the system
	- Node Controller
		- checks the status of the nodes every 5 seconds
		- it waits for 40s before marking a node as unreachable
		- After a node is marked unreachable, it gives it five minutes to come back up, if it doesn't, it removes the PODs assigned to that node
	- Replication Controller
		- is responsible for monitoring the status of replica sets and ensuring the desired number of PODs are available at all times within the set
		- if a POD dies, it creates another one 
	- There are various other controllers
	- Installing kube-controller-manager
		- download the binaries
		- run it
			- kube-controller-manager.service
	- View kube-controller-manager - in a kubeadm setup
		- kubectl get pods -n kube-system
		- cat /etc/kubernetes/manifests/kube-controller-manager.yaml
	- View kube-controller-manager - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-controller-manager.service
		- ps -aux | grep kube-controller-manager


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-Scheduler ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it decides on which Node the PODs to be placed
	- it decides based on some criteria such as resource requirements of PODS
	- here is the procedure of kube-scheduler
		1) Filter Nodes
			- ignore the nodes which don't satisfy the resource limits of the PODs
		2) Rank Nodes
			- it uses a priority function to assign a score to the nodes on a scale of 0 to 10
	- Installing kube-scheduler
		- Download the binaries
		- Extract it
		- Run it as a service
			- kube-scheduler.service
	- View kube-scheduler options - in a kubeadm setup
		- cat /etc/kubernetes/manifests/kube-scheduler.yaml
		- ps -aux | grep kube-scheduler


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kubelet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubelet in the kubernetes worker node, registers the node with the kubernetes cluster
	- it loads or unloads containers on the ship as instructed by the scheduler on the master
	- send back reports on the regular intervals to the master
	- Installing kubelet:
		- download the binaries
		- run it
			- kubelet.service
		* Note: kubeadm does not automatically deploy the kubelet
	- you must always manually install the kubelet on your worker nodes 
	- View kubelet options
		- ps -aux | grep kubelet


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-proxy ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it's a process that runs on each node in the kubernetes cluster
	- its job is to look for new services and every time a new service is created, it creates the appropriate rules on each node to forward traffic to those services to the backend PODs
	- One way it works is using IPTABLES rules
	- Installing kube-proxy
		- download the binaries
		- extract it
		- run it
			- kube-proxy.service
	- View kube-proxy - in kubeadm setup
		- kubectl get pods -n kube-system
		- kubectl get daemonset -n kube-system

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- The containers are encapsulated into a Kubernetes object known as POD
	- A POD is a single instance of an application in the form of a Container
	- A POD is the smallest object that you can create in Kubernetes
	- To scale our Application, we don't create new containers to the existing PODs, we create new PODs
	- deploying pods:
		- e.g.: kubectl run nginx --image nginx
	- Viewing the pods in our Kubernetes cluster
		- kubectl get pods

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs with YAML ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	* Example of a YAML file for creating pod(s)


apiVersion: v1

kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp

spec:
  containers:
    - name: nginx-container
      image: nginx

    - name: backend-container
      image: redis



    - To create PODs using YAML file
    	- kubectl create -f <file_name.yml>



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Replication Controller ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- provides
		- HA
		- Load Balancing & Scaling
	- Replica Set is the new recommended way to setup replication. Replication Controller was the old way to do that

	* Example of defining replication controller using a YAML file (first metadata section relates to Replication Controller, the second metadata section under Template is related to POD definition)


apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3


  	- To create the replication controller using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To view the list of replication controllers
  		- kubectl get replicationcontroller



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ReplicaSet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Example of defining replication controller using a YAML file (Selector section in this file is required)


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 


  	- To create the replicaSet using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To delete the replicaSet
  		- kubectl delete replicaset <ReplicaSet_Name>

  	- To view the list of replication controllers
  		- kubectl get replicaset

  	- Labels and Selectors
  		- These are mechanisms used by replicaSet to know which PODs to monitor, so that it can bring up new PODs in case any of them failed

  	- Scaling ReplicaSet
  		- There are some ways
  			1) update the number of "replicas: <#>" in .yml difinition file
  			   Then use "kubectl replace -f <file_name.yml>" command to update the ReplicaSet

  			2) Use "kubectl scale --replicas=<#> -f <file_name.yml>" to update the ReplicaSet

  			3) Use "kubectl scale --replicas=<#> replicaset <ReplicaSet_Name>"
  				- e.g.: kubectl scale --replicas=6 replicaset myapp-replicaset


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Deployments ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	- Deplolyment provides us with the capability to upgrade the underlying instances seamlessly using
		- rolling updates
		- undo changes
		- pause changes
		- resume changes

	- Deployment Definition file(.yml)
		-  the definition of Deployment is exactly as the definition of ReplicaSet, Except the "kind:" is going to be Deployment

	* Note: The Deployment automatically creates ReplicaSet
 
	* Example of defining Deployment using a YAML file


apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 




  	- To create the Deployment using the YAML file
  		- kubectl create -f <file_name.yml>


  	- To view the list of Deployments
  		- kubectl get deployments


	* Note: To see all the created objects
		- kubectl get all



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Namespaces ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	- There are initially 3 Namespaces created inside Kubernetes
		1) Default
			- it's created automatically in Kubernetes when the Cluster is first set up
			- by default we are in this namespace
		2) kube-system
			- Kubernetes creates a set of Pods and services for its interanl purpose such as networking, DNS, etc
			- To isolate these Pods from the user and prevent from accidentally deleting or modifying these services, Kubernets creates them under this namespace created at cluster startup
		3) kube-public
			- This is where resources that should be made available to all users are created

	* Note: you can create your own namespaces 
	- Each of the namespaces has its own set of policies. For example you can set a certain quota of resources to each namespace
	- The resources inside a namespace can refer to each other simply by their names

	- for refering to the services inside another nameservice, we should append the name of the namespace to the name of the service
		- {service/Pod_name}.{namespace}.svc.cluster.local
		- cluster.local is the default domain name of the Kubernetes cluster
		- SVC is the subdomain for service
		- e.g.: mysql.connect("db-service.dev.svc.cluster.local")

	- To list PODs in a specific namespace
		- kubectl get pods --namespace={namespace_name}
		- e.g.:  kubectl get pods --namespace=kube-system

	- To create a POD using a YAML file in a specific namespace
		- kubectl create -f {Pod-definition-file.yml} --namespace={namespace_name}
		- e.g.:  kubectl create -f pod-definition.yml --namespace=dev
		- you can also place "namespace:" directive in the YAML file under "metadata:" section to make sure your resources are always created inside a specifc namespace


	- To create a namespace using a namespace definition(YAML) file
		- kubectl create -f {namespace-file.yml}

		* Example of a namespace .yml definition file

apiVersion: v1
kind: Namespace
metadata:
    name: dev



	- Another way to create namespace
		- kubectl create namespace {namespace_name}

		
	- To switch the namespace permanently
		- kubectl config set-context $(kubectl config current-context) --namespace={namespace_name}
		- e.g.:  kubectl config set-context $(kubectl config current-context) --namespace=dev

	- To view PODs in all namespaces
		- kubectl get pods --all namespaces

	* Note: Contexts are used to manage multiple clusters in multiple environments from the same management system


	* Example of creating a resource quota for a namespace


apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev

spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi




	* Note: to create a template yaml file from creating a pod or any other object
		- e.g.: kubectl run redis --image=redis --restart=Never --dry-run -o yaml > pod.yaml
		- The above command will create a YAML file which we can edit and then create a pod by the command "kubectl apply -f pod.yaml"


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Services ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Kubernetes Services enable communication between various components within and outside of the application
- Services help us connect applications together with other applications or users


** Service Types
	1) NodePort
		- One of the use cases of Services is to listen to a port on the Node and forward requests on that port to a port on the POD running the application

	2) ClusterIP
		- The service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end services to a set of backend services

	3) LoadBalancer
		- it provides a load balancer for our application in supported cloud providers


** Service - NodePort
	- TargetPort: Pod's Port
	- Port: Service's Port
	- NodePort: Node's Port - can only be in a valid range which by default is 30000-32767
	- If the port is not set manually, it would be set similar to TargetPort
	- If the NodePort is not set manually, a random Port in the valid range would be automatically allocated
	- selector section of the YAML file selects the PODs that will be used for NodePort service

	* Example of defining a NodePort Service via YAML file

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
    
spec:
    type: NodePort
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: front-end

	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



** Service - Cluster IP
	- A Kubernetes Service that can help us group different sets of PODs together and provide a single interface to access the PODs in a group
	- The requests are forwarded to one of the PODs under the service randomly
	- This enables us to easily and effectively deploy a microservices-based application on Kubernetes cluster
	- Each Service gets an IP and Name assigned to it inside the cluster
	- TargetPort in YAML file is the port where the application/POD is exposed
	- Port in YAML file is where the service is exposed
	- To link the service to a set of PODs we use Selector

	* Example of defining a Cluster IP Service via YAML file


apiVersion: v1
kind: Service
metadata:
    name: back-end

spec:
    type: ClusterIP
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: back-end


	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



* Note: to expose port of a Service:
	- e.g.: kubectl expose pod redis --name=redis-service --port=6379 --target-port=6379




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Initialize Kubernetes Master Node with Kubeadm ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 	- In order to initialize a kubernetes cluster with Kubeadm
		- kubeadm  init --pod-network-cidr <cidr>
		- e.g.:
			kubeadm init --pod-network-cidr 10.88.0.0/16

	* Note: If you have already run the kubeadm initialization command and confronted an error, do the following to remove the configs before running the initialization again
		- rm .kube/config

	* validation commands
		- kubectl version
		- kubectl get all --namespace kube-system
		- kubectl get all --namespace default

	* Important common error
		- The connection to the server <Host IP>:6443 was refused - did you specify the right host or port?
		- To solve this error, use the following commands sequentially:
			- sudo nano /etc/fstab
				- Locate the line in the "/etc/fstab" file that specifies the swap partition
				- Comment out this line
			- sudo -i
			- swapoff -a
			- exit
			- strace -eopenat kubectl version
			
	* Installing CNI(Container Network Interface)
		- You must deploy a Container Network Interface (CNI) based Pod network add-on so that your Pods can communicate with each other 
		- Cluster DNS (CoreDNS) will not start up before a network is installed
		- Calico is an open-source CNI
			- Calico has network policy capability
			- to install Calico, go to Calico github page -> getting started section




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Scheduling ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

** Labels and Selectors
   - Labels and selectors are used to group and select objects
   - we can set labels to a pod on the "Labels:" section of Pod definition YAML file:
	- e.g.:

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
      app: App1
      function: Front-end

spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
      - containerPort: 8080

   - to select the labels of the above Pod using a Selector:
     # kubectl get pods --selector app=App1

   * Annotations
     - they are used to record other details for informatory purpose
       - for example tool details like name, version, build information, etc, or contact details(phone number, email, etc.)

   - To get objects with a specific label use "-l" along with label
      - e.g.:
        - kubectl get pods -l env=dev




** Taints and Tolerations
   - Taints and Tolerations are used to set restrictions on what pods can be scheduled on a node
   - Taints are set on Nodes and Tolerations are set on Pods
   - by default pods have no tolerations which means unless specified otherwise, none of the pods can tolerate any taint
   - we add a toleration on a pod to be able to be placed on a node with a specific taint

   - In order to Taint a node:
	# kubectl taint nodes <node_name> key=value:taint-effect
		- taint-effect on the above format defines what would happen to the PODs if they don't tolerate the taint
	- e.g.:
		# kubectl taint nodes node1 app=blue:NoSchedule

   - In order to add Toleration to a POD
	- use the following yaml file format as an example:

apiVersion:
kind: Pod
metadata:
 name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"




   - Taint-effect Types:
	1) NoSchedule
		- The PODs will not be scheduled on the Tainted Node
	2) PreferNoSchedule
		- The system will try to avoid placing a POD on the Tainted Node but that is not guaranteed
	3) NoExecute
		- New Pods will not be scheduled on the node and existing PODs on the Node(if any) will be evicted if they do not tolerate the Taint


   - In order to view the taints on Nodes
	# kubectl describe node <node_name> | grep Taint
	- e.g.:
	# kubectl describe node worker1 | grep Taint





** Node Selectors
   - To label a node use the following format:
     # kubectl label nodes <node-name> <label-key>=<label-value>
     - e.g.:
       # kubectl label nodes node-1 size=large
   - Then we can use the node label on "nodeSelector:" section of Pod definition YAML file(e.g.):

apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

  nodeSelector:
    size: Large


    - Limitation of Node Selector
	- you can not provide advanced expressions like OR or NOT with Node Selectors

	

** Node Affinity
   - The primary purpose of node affinity feature is to ensure that pods are hosted on particular nodes
   - Node Affinity feature provides us with advanced capabilities to limit pod placement on specific nodes
   - Here is an example of using Node Affinity on YAML file:

apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

affinity:
 nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
     nodeSelectorTerms:
     - matchExpressions:
       - key: size
         operator: In
         values:
         - large
	 - medium


OR



apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

affinity:
 nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
     nodeSelectorTerms:
     - matchExpressions:
       - key: size
         operator: Not  In
         values:
         - large
	 - medium



   * Node Affinity Types
      - Available:
	 - requiredDuringSchedulingIgnoreDuringExecution
	   - DuringScheduling: the state where a pod does not exist and is created for the first time
	   - DuringExecution: is the state where a POD has been running and a change is made in the environment that affects node affinity such as a change in the label of a node
	   - required: the scheduler will mandate that the pod be placed on a node with a given affinity rules
	      - if the scheduler cannot find one, the pod will not be scheduled
	      - this type will be used in cases where the placement of the pod is crucial
         - preferredDuringSchedulingIgnoredDuringExecution
	   - preferred: in cases where a matching node is not found, the scheduler will simply ignore node affinity rules and place the Pod on any available node
	       
      - Planned:
	 - requiredDuringSchedulingRequiredDuringExecution



** Resource Requirements and Limits
	- By default Kubernets assumes that a Pod or a Container within a POD requires 0.5 CPU and 256MB of Memory
	- You can change these default resource requirements by specifying them on POD or Deployment definition YAML file
	- Example POD-definition.yaml file:
		* Note: you can set resource limits on "limits" section of the file, as in the following example


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "2"




	- In order to get the Resource Requirements and Limits of a POD
		# kubectl describe pod <pod_name>
		- You can see your desired info on "Containers" section


** Daemon Sets
    - It helps you deploy multiple instances of pods
    - It runs one copy of your POD on each Node in your Cluster
    - Whenever a new Node is added to the Cluster, a Replica of the POD is added to that Node
    - The Daemon Set ensures that one copy of the POD is always present on all Nodes in the Cluster 

    - Use Cases of Daemon Sets
	- Deploying a Monitoring Agent or Log Collector on each of your Nodes in the Cluster
	- The Kube-proxy component can be deployed as a Daemon Set in the Cluster
	- Used for deploying Networking modules like Weave-net or Calico on all the Nodes

    - DaemonSet Definition
	- an example of creating a DaemonSet using a yaml definition file(with "kubectl create -f <definition-file-name>.yaml"):



apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
      - name: monitoring-agent
	image: monitoring-agent





    - In order to view the created DaeomonSet:
	# kubectl get daemonsets
    - To view more details of a DaemonSet:
	# kubectl describe daemonsets <daemonSet-name>




** Monitoring
	- to monitor the resources of kubernetes, download and use "Metrics-Server"
		- e.g.: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
	- then deploy the "Metrics-server" by creating all the components downloaded:
		- e.g.: cd kubernetes-metrics-server
			kubectl create -f .
	- to get the info of the resource-usage:
		- kubectl top node
		- kubectl top podes


** Application Lifecycle Management
	- To see the status of the deployment rollout:
		-e.g.: kubectl rollout status deployment/myapp-deployment

	- To see the revisions and history of Rollouts:
		- e.g.: kubectl rollout history deployment/myapp-deployment

	- Deployment Strategy
		- Recreate
			- in this strategy, all the PODs are removed and then new PODs are created
		- Rolling Update
			- in this strategy, each PODs are shut down and then a new one is created. This process is done sequentially.
		- Note: If the Deployment Strategy not specified, "Rolling Update" is the default strategy

	- Deployment Update methods
		1) configure the changes on the Deployment Definition file and then do "kubectl apply -f [definition.yaml]"
		2) do the change in command line
			- kubectl set image [deployment name] nginx=nginx:1.9.1

	- To undo/Rollback the changes/updates:
		- kubectl rollout undo deployment/myapp-deployment


	* Commands and Arguments
		- In order to set command to be run immediately after POD is run, use "command" and "args" fields on pod-definition.yml file:
			- e.g.:


apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
 containers:
   - name: ubuntu-sleeper
     image: ubuntu-sleeper
     command: ["sleep2.0"]
     args: ["10"]




	* Environment Variables
		- to set an environment variable, use "env:" field in pod-definition.yml file
			- e.g.:
			


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
   - name: simple-webapp-color
     image: simple-webapp-color
     ports:
	   - containerPort: 8080

     env:
	   - name: APP_COLOR
	     value: pink




	* ConfigMaps
		- when you have lots of definition files, it would be difficult to manage the Environment data within the definition files
		- we can store all this environment data centrally on ConfigMap file
		- ConfigMaps are used to pass configuration data in the form of key:value pairs in kubernetes
		- when a pod is created, the ConfigMap is injected to the POD. So key:value pair are available as environment variables for the application hosted inside the container in the pod
		- phases of using ConfigMaps:
			1) create a ConfigMap

				- Imperative
					- kubectl create ConfigMap <config-name> --from-literal=<key>=<value>
					- e.g.: kubectl create ConfigMap app-config --from-literal=APP_COLOR=blue \
																--from-literal=APP_MOD=prod

					OR
					- kubectl create configmap <config-name> --from-file=<path-to-file>
					- e.g.: kubectl create configmap \
							app-config --from-file=app_config.properties

				- Declarative
					-kubectl create -f config-map.yaml
					- config-map.yaml file contents:

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

			

			2) inject a ConfigMap in a POD-Definition file:
				- config-map.yaml file contents:

				apiVersion: v1
				kind: ConfigMap
				metadata:
				name: app-config
				data:
				APP_COLOR: blue
				APP_MODE: prod

				- pod-definition.yaml file contents:


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
     image: simple-webapp-color
     ports:
	   - containerPort: 8080

     envFrom:
	   - configMapRef:
	     	name: app-config




		- To view ConfigMaps
				- kubectl get configmaps
		- To get the details of a ConfigMap
				- kubectl describe configmaps

	* Secrets
		- Phases of using Secrets:
			1) Creating Secret
				- example of Secret file

DB_Host: mysql
DB_User: root
DB_Password: passwrd


				- Imperative:
					- kubectl create secret generic <secret-name> --from-literal=<key>=<value>
					- e.g.: kubectl create secret generic \
					            app-secret --from-literal=DB_Host=mysql

					OR
					- kubectl create secret generic <secret-name> --from-file=<path-to-file>
					- e.g.: kubectl create secret generic app-secret --from-file=app_secret.properties
				
				- Declarative
					- e.g.: kubectl create -f secret-data.yaml
					- contents of secret-data.yaml file:


apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  Data_Host: mysql
  DB_User: root
  DB_Password: passwrd



		- To view Secrets
				- kubectl get secrets
		- To get the details of a secret
				- kubectl describe secrets


			2) inject a Secret in a POD-Definition file:
				- secret file contents:

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  Data_Host: mysql
  DB_User: root
  DB_Password: passwrd


				- - pod-definition.yaml file contents:

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
   image: simple-webapp-color
   ports:
	 - containerPort: 8080

envFrom:
- secretRef:
	name: app-secret


	

	* Multi Container Pods
		- example of pod-definition.yaml file with two containers:


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
	name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp-color
    ports:
   	  - containerPort: 8080
  - name: log-agent
    image: log-agent



** Cluster Maintenance
	- Draining a Node
		- when you drain a node, the PODs are gracefully terminated from the node they run on and they get recreated on another node. The node also is marked as "Unschedulable",
		  meaning no PODs can be scheduled on this node
	- Uncordoning a node
		- To make a node "schedulable" again after we have drained it, we should "undordon" it
			- e.g.:  kubectl uncordon node-1
	- Cordoning a node
		- "cordon" simply marks a node "Unschedulable", however it doesn't move the existing PODs to another node, it simply prevents the new PODs to be scheduled on this node

	- Cluster Upgrade process
		 - The components of Kubernetes can be at different versions
		 - None of the Kubernetes core components(Controller-manager, kube-scheduler, kubelet, kube-proxy) can be in a higher version than "kube-apiserver" version
		 - Kubectl can be at a version higher than the version of kube-apiserver
		 - The recommended upgrade path is to upgrade one minor version at a time
		 - Upgrading a cluster involves 2 major steps:
		 	1) Upgrade the Master nodes
				- While upgrading the master nodes, the control plane components go down and the worker nodes are not affected
			2) Upgrade the Worker nodes 
				- strategies:
					1) Upgrading all of the worker nodes at once
						- In this case, all the PODs are down
						- this strategy requires down-time
					2) Upgrade one node at a time
						- No down-time is required
					3) Add new Nodes(with newer software version) to the cluster
						- Then the workloads are moved to the new Nodes, and old Nodes are removed
		 - Kubeadm has upgrade command to help upgrading the cluster:
		 	- Kubeadm upgrade plan
				- It provides the current cluster version, kubeadm version, latest stable version of K8S
				- Also provides all the control plane components' current and available versions
				- After upgrading the Kubeadm components, you must manually upgrade the kubelet on each node
				- Remember: Kubeadm doesn't automatically update or install kubelet
				- It also gives you the command to upgrade the cluster
				- Note: Before being able to upgrade the cluster, you must manually upgrade the kubeadm
				- Note: Kubeadm also follows the same software version as K8S
				- Note: The worker nodes should be drained before getting upgraded. Here is the drain and upgrade command example on a worker node:
					- execute on master node:
						- kubectl drain node-1
					- execute on worker node:
						- apt-get upgrade -y kubeadm=1.12.0-00
						- apt-get upgrade -y kubelet=1.12.0-00
						- kubeadm upgrade node config --kubelet-version v1.12.0
						- systemctl restart kubelet
					- execute on master node:
						- kubectl uncordon node=1


