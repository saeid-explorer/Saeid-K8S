============================================================================================ KUBERNETES CKA Course - UDEMY_ MUMSHAD ======================================================================================================

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Requirements after installing Kubernetes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	# sudo systemctl stop ufw && sudo systemctl disable ufw
	# echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf
	# sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab && sudo swapoff -a
	# export KUBECONFIG=/etc/kubernetes/admin.conf
	- verify if kube api server is running:
		# ps aux | grep kube-apiserver	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Cluster Architecture~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Note: The purpose of Kubernetes is to host your applications in the form of containers in an automated fasion so that you can deploy as many instances of your application and easily enable communication between different services within your application.

	- Types of Nodes and their components in Kubernetes
		
		- Master Nodes: Manage, Plan, Schedule, Monitor Nodes
			
			- Etcd Cluster
				- Etcd is a database that stores information in a key-value format

			- Kube-Scheduler
				- A scheduler identifies the right node to place a container on based on the containers reousrce requirements, the worker nodes capacity or other constraints

			- Controller Manager
				- Node-Controller
					- Takes care of nodes. Responsible for onboarding new nodes to the cluster handling situations where nodes become unavailable
				- Replication-Controller
					- Ensures that the disired number of containers are running at all times in your replication group 
				
			- Kube-api server
				- It's the primary management component of kubernetes
				- It's responsible for orchestrating all operations within the cluster
				- It Exposes the Kubernetes API which is used for external users to perform management operations on the cluster 


		- Worker Nodes: Host Application as Containers
				
			- Container Runtime Engine
				- Softwares that can run containers
				- Popular ones are Docker, Containerd, Rocket
				- Container Runtime Engine must be installed on all the nodes in the cluster(including the Master nodes)

			- Kubelet
				- A kubelet is an agent that runs on each node in a cluster
				- It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required
				- Kube-api server priodically fetches status reports from the kubelet to monitor the state of nodes and containers on them

			- kube-proxy
				- Communication between worker nodes are enable by this on the worker node
				- Ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ETCD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- ETCD is a distributed reliable key-value store that is simple, secure & fast
	- Installing
		1) Download Binaries
		2) Extract
			- tar xzvf <filename.tar.gz>
		3) Run ETCD Service
			- ./etcd
	- ETCD service listens on port 2379 by default
	- The default client that comes with ETCD is ETCD control client(etcdctl)
		- You can use it to store and retrieve key-value pairs
			- e.g.:  ./etcdctl set key1 value1
			- e.g.:  ./etcdctl get key1
		- getting help
			- ./etcdctl
	- ETC includes the following data:
		- Nodes
		- PODs
		- Configs
		- Secrets
		- Accounts
		- Roles
		- Bindings
		- Others
		- and Every Change on the cluster

* Note: The practice test environments are deployed using the kubeadm tool 

	- Setup ETCD when you deploy a cluster from scratch
		- first you download the binaries of ETCD
			- e.g.:
				- wget -q --https-only "https://github.com/coreos/etcd/releases/download/v3.5.13/etcd-v3.5.13-linux-amd64.tar.gz"
		* Note: --advertise-client-url: it's the address on which ETCD listens{Server_IP:2379}

	- Setup ETC when you deoploy a cluster using kubeadm tool
		- if you setup your cluster using kubeadm, then kubeadm deploys the ETCD server for you as a POD in the kube-system namespace
		- kubectl get pods -n kube-system
		- To list all keys stored on ETCD by Kubernetes:
			- kubectl exec etcd-master -n kube-system etcdctl get / --prefix --keys-only

	- ETCD stores data in a registry and under that there are various Kubernets constructs such as nodes, pods, replicasets, etc.

	- ETCD in HA Environment
		- in a multi-master Kubernetes Environment we should make sure the different ETCD services know about each other
			- --initial-cluster option in etcd.service defines ETCDs on a cluster


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-API ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubectl command in fact reaches to kube-api server (e.g. kubectl get nodes )
	- kubeapi first authenticates the request and validates it
	- it then retrieves the data from ETCD Cluster and responds back with the requested information
	- Actions as a result of a change request(e.g. creating a POD) in the cluster
		- Authenticate User
		- Validate Request
		- Retrieve data
		- Update ETCD
		- Scheduler
		- Kubelet

	- viewing api-server(kube-api) - when you deploy cluster using kubeadm
		- kubectl get pods -n kube-system 
		OR
		- cat /etc/kubernetes/manifests/kube-apiserver.yaml

	- viewing api-server(kube-api) - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-apiserver.service
		- ps -aux | grep kube-apiserver



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube Controller Manager ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- a Controller is a process that continuously monitors the state of various components within the system
	- Node Controller
		- checks the status of the nodes every 5 seconds
		- it waits for 40s before marking a node as unreachable
		- After a node is marked unreachable, it gives it five minutes to come back up, if it doesn't, it removes the PODs assigned to that node
	- Replication Controller
		- is responsible for monitoring the status of replica sets and ensuring the desired number of PODs are available at all times within the set
		- if a POD dies, it creates another one 
	- There are various other controllers
	- Installing kube-controller-manager
		- download the binaries
		- run it
			- kube-controller-manager.service
	- View kube-controller-manager - in a kubeadm setup
		- kubectl get pods -n kube-system
		- cat /etc/kubernetes/manifests/kube-controller-manager.yaml
	- View kube-controller-manager - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-controller-manager.service
		- ps -aux | grep kube-controller-manager


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-Scheduler ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it decides on which Node the PODs to be placed
	- it decides based on some criteria such as resource requirements of PODS
	- here is the procedure of kube-scheduler
		1) Filter Nodes
			- ignore the nodes which don't satisfy the resource limits of the PODs
		2) Rank Nodes
			- it uses a priority function to assign a score to the nodes on a scale of 0 to 10
	- Installing kube-scheduler
		- Download the binaries
		- Extract it
		- Run it as a service
			- kube-scheduler.service
	- View kube-scheduler options - in a kubeadm setup
		- cat /etc/kubernetes/manifests/kube-scheduler.yaml
		- ps -aux | grep kube-scheduler


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kubelet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubelet in the kubernetes worker node, registers the node with the kubernetes cluster
	- it loads or unloads containers on the ship as instructed by the scheduler on the master
	- send back reports on the regular intervals to the master
	- Installing kubelet:
		- download the binaries
		- run it
			- kubelet.service
		* Note: kubeadm does not automatically deploy the kubelet
	- you must always manually install the kubelet on your worker nodes 
	- View kubelet options
		- ps -aux | grep kubelet


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-proxy ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it's a process that runs on each node in the kubernetes cluster
	- its job is to look for new services and every time a new service is created, it creates the appropriate rules on each node to forward traffic to those services to the backend PODs
	- One way it works is using IPTABLES rules
	- Installing kube-proxy
		- download the binaries
		- extract it
		- run it
			- kube-proxy.service
	- View kube-proxy - in kubeadm setup
		- kubectl get pods -n kube-system
		- kubectl get daemonset -n kube-system

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- The containers are encapsulated into a Kubernetes object known as POD
	- A POD is a single instance of an application in the form of a Container
	- A POD is the smallest object that you can create in Kubernetes
	- To scale our Application, we don't create new containers to the existing PODs, we create new PODs
	- deploying pods:
		- e.g.: kubectl run nginx --image nginx
	- Viewing the pods in our Kubernetes cluster
		- kubectl get pods

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs with YAML ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	* Example of a YAML file for creating pod(s)


apiVersion: v1

kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp

spec:
  containers:
    - name: nginx-container
      image: nginx

    - name: backend-container
      image: redis



    - To create PODs using YAML file
    	- kubectl create -f <file_name.yml>



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Replication Controller ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- provides
		- HA
		- Load Balancing & Scaling
	- Replica Set is the new recommended way to setup replication. Replication Controller was the old way to do that

	* Example of defining replication controller using a YAML file (first metadata section relates to Replication Controller, the second metadata section under Template is related to POD definition)


apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3


  	- To create the replication controller using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To view the list of replication controllers
  		- kubectl get replicationcontroller



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ReplicaSet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Example of defining replication controller using a YAML file (Selector section in this file is required)


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 


  	- To create the replicaSet using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To delete the replicaSet
  		- kubectl delete replicaset <ReplicaSet_Name>

  	- To view the list of replication controllers
  		- kubectl get replicaset

  	- Labels and Selectors
  		- These are mechanisms used by replicaSet to know which PODs to monitor, so that it can bring up new PODs in case any of them failed

  	- Scaling ReplicaSet
  		- There are some ways
  			1) update the number of "replicas: <#>" in .yml difinition file
  			   Then use "kubectl replace -f <file_name.yml>" command to update the ReplicaSet

  			2) Use "kubectl scale --replicas=<#> -f <file_name.yml>" to update the ReplicaSet

  			3) Use "kubectl scale --replicas=<#> replicaset <ReplicaSet_Name>"
  				- e.g.: kubectl scale --replicas=6 replicaset myapp-replicaset


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Deployments ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	- Deplolyment provides us with the capability to upgrade the underlying instances seamlessly using
		- rolling updates
		- undo changes
		- pause changes
		- resume changes

	- Deployment Definition file(.yml)
		-  the definition of Deployment is exactly as the definition of ReplicaSet, Except the "kind:" is going to be Deployment

	* Note: The Deployment automatically creates ReplicaSet
 
	* Example of defining Deployment using a YAML file


apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 




  	- To create the Deployment using the YAML file
  		- kubectl create -f <file_name.yml>


  	- To view the list of Deployments
  		- kubectl get deployments


	* Note: To see all the created objects
		- kubectl get all



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Namespaces ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	- There are initially 3 Namespaces created inside Kubernetes
		1) Default
			- it's created automatically in Kubernetes when the Cluster is first set up
			- by default we are in this namespace
		2) kube-system
			- Kubernetes creates a set of Pods and services for its interanl purpose such as networking, DNS, etc
			- To isolate these Pods from the user and prevent from accidentally deleting or modifying these services, Kubernets creates them under this namespace created at cluster startup
		3) kube-public
			- This is where resources that should be made available to all users are created

	* Note: you can create your own namespaces 
	- Each of the namespaces has its own set of policies. For example you can set a certain quota of resources to each namespace
	- The resources inside a namespace can refer to each other simply by their names

	- for refering to the services inside another nameservice, we should append the name of the namespace to the name of the service
		- {service/Pod_name}.{namespace}.svc.cluster.local
		- cluster.local is the default domain name of the Kubernetes cluster
		- SVC is the subdomain for service
		- e.g.: mysql.connect("db-service.dev.svc.cluster.local")

	- To list PODs in a specific namespace
		- kubectl get pods --namespace={namespace_name}
		- e.g.:  kubectl get pods --namespace=kube-system

	- To create a POD using a YAML file in a specific namespace
		- kubectl create -f {Pod-definition-file.yml} --namespace={namespace_name}
		- e.g.:  kubectl create -f pod-definition.yml --namespace=dev
		- you can also place "namespace:" directive in the YAML file under "metadata:" section to make sure your resources are always created inside a specifc namespace


	- To create a namespace using a namespace definition(YAML) file
		- kubectl create -f {namespace-file.yml}

		* Example of a namespace .yml definition file

apiVersion: v1
kind: Namespace
metadata:
    name: dev



	- Another way to create namespace
		- kubectl create namespace {namespace_name}

		
	- To switch the namespace permanently
		- kubectl config set-context $(kubectl config current-context) --namespace={namespace_name}
		- e.g.:  kubectl config set-context $(kubectl config current-context) --namespace=dev

	- To view PODs in all namespaces
		- kubectl get pods --all namespaces

	* Note: Contexts are used to manage multiple clusters in multiple environments from the same management system


	* Example of creating a resource quota for a namespace


apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev

spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi




	* Note: to create a template yaml file from creating a pod or any other object
		- e.g.: kubectl run redis --image=redis --restart=Never --dry-run -o yaml > pod.yaml
		- The above command will create a YAML file which we can edit and then create a pod by the command "kubectl apply -f pod.yaml"


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Services ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Kubernetes Services enable communication between various components within and outside of the application
- Services help us connect applications together with other applications or users


** Service Types
	1) NodePort
		- One of the use cases of Services is to listen to a port on the Node and forward requests on that port to a port on the POD running the application

	2) ClusterIP
		- The service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end services to a set of backend services

	3) LoadBalancer
		- it provides a load balancer for our application in supported cloud providers


** Service - NodePort
	- TargetPort: Pod's Port
	- Port: Service's Port
	- NodePort: Node's Port - can only be in a valid range which by default is 30000-32767
	- If the port is not set manually, it would be set similar to TargetPort
	- If the NodePort is not set manually, a random Port in the valid range would be automatically allocated
	- selector section of the YAML file selects the PODs that will be used for NodePort service

	* Example of defining a NodePort Service via YAML file

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
    
spec:
    type: NodePort
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: front-end

	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



** Service - Cluster IP
	- A Kubernetes Service that can help us group different sets of PODs together and provide a single interface to access the PODs in a group
	- The requests are forwarded to one of the PODs under the service randomly
	- This enables us to easily and effectively deploy a microservices-based application on Kubernetes cluster
	- Each Service gets an IP and Name assigned to it inside the cluster
	- TargetPort in YAML file is the port where the application/POD is exposed
	- Port in YAML file is where the service is exposed
	- To link the service to a set of PODs we use Selector

	* Example of defining a Cluster IP Service via YAML file


apiVersion: v1
kind: Service
metadata:
    name: back-end

spec:
    type: ClusterIP
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: back-end


	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



* Note: to expose port of a Service:
	- e.g.: kubectl expose pod redis --name=redis-service --port=6379 --target-port=6379




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Initialize Kubernetes Master Node with Kubeadm ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 	- In order to initialize a kubernetes cluster with Kubeadm
		- kubeadm  init --pod-network-cidr <cidr>
		- e.g.:
			kubeadm init --pod-network-cidr 10.88.0.0/16

	* Note: If you have already run the kubeadm initialization command and confronted an error, do the following to remove the configs before running the initialization again
		- rm .kube/config

	* validation commands
		- kubectl version
		- kubectl get all --namespace kube-system
		- kubectl get all --namespace default

	* Important common error
		- The connection to the server <Host IP>:6443 was refused - did you specify the right host or port?
		- To solve this error, use the following commands sequentially:
			- sudo nano /etc/fstab
				- Locate the line in the "/etc/fstab" file that specifies the swap partition
				- Comment out this line
			- sudo -i
			- swapoff -a
			- exit
			- strace -eopenat kubectl version
			
	* Installing CNI(Container Network Interface)
		- You must deploy a Container Network Interface (CNI) based Pod network add-on so that your Pods can communicate with each other 
		- Cluster DNS (CoreDNS) will not start up before a network is installed
		- Calico is an open-source CNI
			- Calico has network policy capability
			- to install Calico, go to Calico github page -> getting started section




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Scheduling ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

** Labels and Selectors
   - Labels and selectors are used to group and select objects
   - we can set labels to a pod on the "Labels:" section of Pod definition YAML file:
	- e.g.:

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
      app: App1
      function: Front-end

spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
      - containerPort: 8080

   - to select the labels of the above Pod using a Selector:
     # kubectl get pods --selector app=App1

   * Annotations
     - they are used to record other details for informatory purpose
       - for example tool details like name, version, build information, etc, or contact details(phone number, email, etc.)

   - To get objects with a specific label use "-l" along with label
      - e.g.:
        - kubectl get pods -l env=dev




** Taints and Tolerations
   - Taints and Tolerations are used to set restrictions on what pods can be scheduled on a node
   - Taints are set on Nodes and Tolerations are set on Pods
   - by default pods have no tolerations which means unless specified otherwise, none of the pods can tolerate any taint
   - we add a toleration on a pod to be able to be placed on a node with a specific taint

   - In order to Taint a node:
	# kubectl taint nodes <node_name> key=value:taint-effect
		- taint-effect on the above format defines what would happen to the PODs if they don't tolerate the taint
	- e.g.:
		# kubectl taint nodes node1 app=blue:NoSchedule

   - In order to add Toleration to a POD
	- use the following yaml file format as an example:

apiVersion:
kind: Pod
metadata:
 name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"




   - Taint-effect Types:
	1) NoSchedule
		- The PODs will not be scheduled on the Tainted Node
	2) PreferNoSchedule
		- The system will try to avoid placing a POD on the Tainted Node but that is not guaranteed
	3) NoExecute
		- New Pods will not be scheduled on the node and existing PODs on the Node(if any) will be evicted if they do not tolerate the Taint


   - In order to view the taints on Nodes
	# kubectl describe node <node_name> | grep Taint
	- e.g.:
	# kubectl describe node worker1 | grep Taint





** Node Selectors
   - To label a node use the following format:
     # kubectl label nodes <node-name> <label-key>=<label-value>
     - e.g.:
       # kubectl label nodes node-1 size=large
   - Then we can use the node label on "nodeSelector:" section of Pod definition YAML file(e.g.):

apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

  nodeSelector:
    size: Large


    - Limitation of Node Selector
	- you can not provide advanced expressions like OR or NOT with Node Selectors

	

** Node Affinity
   - The primary purpose of node affinity feature is to ensure that pods are hosted on particular nodes
   - Node Affinity feature provides us with advanced capabilities to limit pod placement on specific nodes
   - Here is an example of using Node Affinity on YAML file:

apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

affinity:
 nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
     nodeSelectorTerms:
     - matchExpressions:
       - key: size
         operator: In
         values:
         - large
	 - medium


OR



apiVersion:
Kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor

affinity:
 nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
     nodeSelectorTerms:
     - matchExpressions:
       - key: size
         operator: Not  In
         values:
         - large
	 - medium



   * Node Affinity Types
      - Available:
	 - requiredDuringSchedulingIgnoreDuringExecution
	   - DuringScheduling: the state where a pod does not exist and is created for the first time
	   - DuringExecution: is the state where a POD has been running and a change is made in the environment that affects node affinity such as a change in the label of a node
	   - required: the scheduler will mandate that the pod be placed on a node with a given affinity rules
	      - if the scheduler cannot find one, the pod will not be scheduled
	      - this type will be used in cases where the placement of the pod is crucial
         - preferredDuringSchedulingIgnoredDuringExecution
	   - preferred: in cases where a matching node is not found, the scheduler will simply ignore node affinity rules and place the Pod on any available node
	       
      - Planned:
	 - requiredDuringSchedulingRequiredDuringExecution



** Resource Requirements and Limits
	- By default Kubernets assumes that a Pod or a Container within a POD requires 0.5 CPU and 256MB of Memory
	- You can change these default resource requirements by specifying them on POD or Deployment definition YAML file
	- Example POD-definition.yaml file:
		* Note: you can set resource limits on "limits" section of the file, as in the following example


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "2"




	- In order to get the Resource Requirements and Limits of a POD
		# kubectl describe pod <pod_name>
		- You can see your desired info on "Containers" section


** Daemon Sets
    - It helps you deploy multiple instances of pods
    - It runs one copy of your POD on each Node in your Cluster
    - Whenever a new Node is added to the Cluster, a Replica of the POD is added to that Node
    - The Daemon Set ensures that one copy of the POD is always present on all Nodes in the Cluster 

    - Use Cases of Daemon Sets
	- Deploying a Monitoring Agent or Log Collector on each of your Nodes in the Cluster
	- The Kube-proxy component can be deployed as a Daemon Set in the Cluster
	- Used for deploying Networking modules like Weave-net or Calico on all the Nodes

    - DaemonSet Definition
	- an example of creating a DaemonSet using a yaml definition file(with "kubectl create -f <definition-file-name>.yaml"):



apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
      - name: monitoring-agent
	image: monitoring-agent





    - In order to view the created DaeomonSet:
	# kubectl get daemonsets
    - To view more details of a DaemonSet:
	# kubectl describe daemonsets <daemonSet-name>




** Monitoring
	- to monitor the resources of kubernetes, download and use "Metrics-Server"
		- e.g.: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
	- then deploy the "Metrics-server" by creating all the components downloaded:
		- e.g.: cd kubernetes-metrics-server
			kubectl create -f .
	- to get the info of the resource-usage:
		- kubectl top node
		- kubectl top podes


** Application Lifecycle Management
	- To see the status of the deployment rollout:
		-e.g.: kubectl rollout status deployment/myapp-deployment

	- To see the revisions and history of Rollouts:
		- e.g.: kubectl rollout history deployment/myapp-deployment

	- Deployment Strategy
		- Recreate
			- in this strategy, all the PODs are removed and then new PODs are created
		- Rolling Update
			- in this strategy, each PODs are shut down and then a new one is created. This process is done sequentially.
		- Note: If the Deployment Strategy not specified, "Rolling Update" is the default strategy

	- Deployment Update methods
		1) configure the changes on the Deployment Definition file and then do "kubectl apply -f [definition.yaml]"
		2) do the change in command line
			- kubectl set image [deployment name] nginx=nginx:1.9.1

	- To undo/Rollback the changes/updates:
		- kubectl rollout undo deployment/myapp-deployment


	* Commands and Arguments
		- In order to set command to be run immediately after POD is run, use "command" and "args" fields on pod-definition.yml file:
			- e.g.:


apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
 containers:
   - name: ubuntu-sleeper
     image: ubuntu-sleeper
     command: ["sleep2.0"]
     args: ["10"]




	* Environment Variables
		- to set an environment variable, use "env:" field in pod-definition.yml file
			- e.g.:
			


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
   - name: simple-webapp-color
     image: simple-webapp-color
     ports:
	   - containerPort: 8080

     env:
	   - name: APP_COLOR
	     value: pink




	* ConfigMaps
		- when you have lots of definition files, it would be difficult to manage the Environment data within the definition files
		- we can store all this environment data centrally on ConfigMap file
		- ConfigMaps are used to pass configuration data in the form of key:value pairs in kubernetes
		- when a pod is created, the ConfigMap is injected to the POD. So key:value pair are available as environment variables for the application hosted inside the container in the pod
		- phases of using ConfigMaps:
			1) create a ConfigMap

				- Imperative
					- kubectl create ConfigMap <config-name> --from-literal=<key>=<value>
					- e.g.: kubectl create ConfigMap app-config --from-literal=APP_COLOR=blue \
																--from-literal=APP_MOD=prod

					OR
					- kubectl create configmap <config-name> --from-file=<path-to-file>
					- e.g.: kubectl create configmap \
							app-config --from-file=app_config.properties

				- Declarative
					-kubectl create -f config-map.yaml
					- config-map.yaml file contents:

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

			

			2) inject a ConfigMap in a POD-Definition file:
				- config-map.yaml file contents:

				apiVersion: v1
				kind: ConfigMap
				metadata:
				name: app-config
				data:
				APP_COLOR: blue
				APP_MODE: prod

				- pod-definition.yaml file contents:


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
     image: simple-webapp-color
     ports:
	   - containerPort: 8080

     envFrom:
	   - configMapRef:
	     	name: app-config




		- To view ConfigMaps
				- kubectl get configmaps
		- To get the details of a ConfigMap
				- kubectl describe configmaps

	* Secrets
		- Phases of using Secrets:
			1) Creating Secret
				- example of Secret file

DB_Host: mysql
DB_User: root
DB_Password: passwrd


				- Imperative:
					- kubectl create secret generic <secret-name> --from-literal=<key>=<value>
					- e.g.: kubectl create secret generic \
					            app-secret --from-literal=DB_Host=mysql

					OR
					- kubectl create secret generic <secret-name> --from-file=<path-to-file>
					- e.g.: kubectl create secret generic app-secret --from-file=app_secret.properties
				
				- Declarative
					- e.g.: kubectl create -f secret-data.yaml
					- contents of secret-data.yaml file:


apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  Data_Host: mysql
  DB_User: root
  DB_Password: passwrd



		- To view Secrets
				- kubectl get secrets
		- To get the details of a secret
				- kubectl describe secrets


			2) inject a Secret in a POD-Definition file:
				- secret file contents:

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  Data_Host: mysql
  DB_User: root
  DB_Password: passwrd


				- - pod-definition.yaml file contents:

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
   image: simple-webapp-color
   ports:
	 - containerPort: 8080

envFrom:
- secretRef:
	name: app-secret


	

	* Multi Container Pods
		- example of pod-definition.yaml file with two containers:


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
	name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp-color
    ports:
   	  - containerPort: 8080
  - name: log-agent
    image: log-agent



** Cluster Maintenance
	- Draining a Node
		- when you drain a node, the PODs are gracefully terminated from the node they run on and they get recreated on another node. The node also is marked as "Unschedulable",
		  meaning no PODs can be scheduled on this node
	- Uncordoning a node
		- To make a node "schedulable" again after we have drained it, we should "undordon" it
			- e.g.:  kubectl uncordon node-1
	- Cordoning a node
		- "cordon" simply marks a node "Unschedulable", however it doesn't move the existing PODs to another node, it simply prevents the new PODs to be scheduled on this node

	- Cluster Upgrade process
		 - The components of Kubernetes can be at different versions
		 - None of the Kubernetes core components(Controller-manager, kube-scheduler, kubelet, kube-proxy) can be in a higher version than "kube-apiserver" version
		 - Kubectl can be at a version higher than the version of kube-apiserver
		 - The recommended upgrade path is to upgrade one minor version at a time
		 - Upgrading a cluster involves 2 major steps:
		 	1) Upgrade the Master nodes
				- While upgrading the master nodes, the control plane components go down and the worker nodes are not affected
			2) Upgrade the Worker nodes 
				- strategies:
					1) Upgrading all of the worker nodes at once
						- In this case, all the PODs are down
						- this strategy requires down-time
					2) Upgrade one node at a time
						- No down-time is required
					3) Add new Nodes(with newer software version) to the cluster
						- Then the workloads are moved to the new Nodes, and old Nodes are removed
		 - Kubeadm has upgrade command to help upgrading the cluster:
		 	- Kubeadm upgrade plan
				- It provides the current cluster version, kubeadm version, latest stable version of K8S
				- Also provides all the control plane components' current and available versions
				- After upgrading the Kubeadm components, you must manually upgrade the kubelet on each node
				- Remember: Kubeadm doesn't automatically update or install kubelet
				- It also gives you the command to upgrade the cluster
				- Note: Before being able to upgrade the cluster, you must manually upgrade the kubeadm
				- Note: Kubeadm also follows the same software version as K8S
				- Note: The worker nodes should be drained before getting upgraded. Here is the drain and upgrade command example on a worker node:
					- execute on master node:
						- kubectl drain node-1
					- execute on worker node:
						- apt-get upgrade -y kubeadm=1.12.0-00
						- apt-get upgrade -y kubelet=1.12.0-00
						- kubeadm upgrade node config --kubelet-version v1.12.0
						- systemctl restart kubelet
					- execute on master node:
						- kubectl uncordon node=1

	* Backup & Restore
		- Backup candidates
			- Resource configuration
				- one method is like the following:
					- kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml
			- ETCD Cluster
				- To backup the etcd cluster:
					- ETCDCTL_API=3 etcdctl \
						snapshot save snapshot.db
				- To restore the backup
					- service kube-apiserver stop
					- ETCDCTL_API=3 etcdctl \
						snapshot restore snapshot.db \
						--data-dir /var/lib/etcd-from-backup \
						--initial-cluster master-1=https://192.168.5.11:2380,master-2=https://192.168.5.12:2380 \
						--initial-cluster-token etcd-cluster-1 \
						--initial-advertise-peer-urls https://${INTERNAL_IP}:2380
			- Persistent Volumes



** Security

	* Securing Hosts
		- Password based authentication disabled
		- SSH Key based authentication to be used
		- Authentication
			- Files - Username and Passwords
			- Files - Username and Tokens
			- Certificates
			- External Authentication providers - LDAP
			- Service Accounts
		- Autorization
			- RBAC Authorization
			- ABAC Autorization
			- Node Authorization
			- Webhook Mode

	* Authentication
		- All user access is managed by kube-apiserver
		- Authentication mechanisms
			- Static Password Files
				-   create a csv file including the users and passwords and set it in "kube-apiserver.service" file as follows:
						--basic-auth-file=user-detail.csv
				- To authenticate using the basic credentials in accessing the API server, specify user:password in curl command as the following example:
					- curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123"
				- This is not a recommended authentication mechanisms
				- Consider volume mount while providing the auth file in a kubeadm setup
			- Static Token Files
				-   create a csv file including the users and tokens and set it in "kube-apiserver.service" file as follows:
						--token-auth-file=user-detail.csv
				- To authenticate using the basic credentials in accessing the API server, specify user:token in curl command as the following example:
					- curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer KpjCVbI7rCFAHYPkBzRb7gu1cUc4B"
				- This is not a recommended authentication mechanisms
				- Consider volume mount while providing the auth file in a kubeadm setup
			- Certificates
				- example of Creating Public-Private Key pair:
					- openssl genrsa -out my-bank.key 1024
					- Creating public key from an input private key:
						- openssl rsa -in my-bank.key -pubout > mybank.pem
				- Requesting a Certificate from a CSA(Certificate Signing Authority). This request is called CSR(Certificate Signing Request):
					- openssl req -new -key my-bank.key -out my-bank.csr -subj "/C=US/ST=CA/O=MyOrg,Inc./CN=my-bank.com"
				- Certificate Authority
					- Generate keys
						- openssl genrsa -out ca.key 2048
					- Certificate Signing Request
						- openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
					- Sign Certificates
						- openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
				- ADMIN User
					- Generate keys
						- openssl genrsa -out admin.key 2048
					- Certificate Signing Request
						- openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr
					- Sign Certificates
						- openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
				- To use Certificate when communicating with kube-apiserver
					- 1) you can specify the certificate in curl command:
						- e.g.: curl https://kube-apiserver:6443/api/v1/pods \
									--key admin.key --cert admin.crt --cacert ca.crt
					2) you can specify the certificate in kube-config.yaml
						- e.g.:


apiVersion: v1
clusters:
- cluster:
	certificate-authority: ca.crt
	server: https://kube-apiserver:6443
  name: kubernetes
kind: Config
users:
- name: kubernetes-admin
  user:
    client-certificate: admin.crt
	client-key: admin.key



	* View Certificates in an existing cluster
		- check the following folder if you have deployed K8S using Kubeadm:
			- cat /etc/kubernetes/manifests/kube-apiserver.yaml
		- Then take each certificate (e.g. /etc/kubernetes/pki/apiserver.crt) and look inside it:
			-  openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
		- Also check the service details:
			- in case you've deployed K8S from scratch
				- journalctl -u etcd.service -l
			- in case you've deployed K8S using Kubeadm
				- kubectl logs etcd-master
	

	* Certificate Workflow & API
		- Using Certificates API, Admin will:
			1) Create CertificateSigningRequest object
			e.g.:
			
- jane-csr.yaml:

apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: jane
spec:
  groups:
  - system:authenticated
  usages:
  - digital signature
  - key encipherment
  - server auth
  request:
		[base64 of .csr file contents]

	
			2) Review Requests
				- kubectl get csr
			3) Approve Requests
				- kubectl certificate approve jane
				* Note: to view the certificate:
					- kubectl get csr jane -o yaml
			4) Share Certs to Users
				- first decode the certificate section of the certificate:
					- echo "[certificate]" | base64 --decode
					- then share with the end-user

	*Note: All the certificate-related operations are carried out by "Controller Manager"
			


	* KUBECONFIG
		- using certificates in kubectl command:
			- e.g.:
				- kubectl get pods
					--server my-kube-playground:6443
					--client-key admin.key
					--client-certificate admin.crt
					--certificate-authority ca.crt
		- instead of specifying the certificates everytime in commandline, we can move the above fields to
			the kubeconfig file and then specify this file as kubeconfig option in our command:
				- kubectl get pods --kubeconfig config
		-by default kubeconfig file is a file named "config" with this path: $HOME/.kube/config
		- kubeconfig structure:
			- It has three sections:
				- Clusters
				- Contexts
					- context defines which user account will be used to access which cluster
				- Users
		
		
		- example kubeconfig file:


apiVersion: v1
kind: Config

clusters:

- name: my-kube-playground
  cluster:
    certificate-authority: ca.cert
    server: https://my-kube-playground:6443

contexts:

- name: my-kube-admin@my-kube-playground
  context:
    cluster: my-kube-playground
	user: my-kube-admin

users:

- name: my-kube-admin
  user:
    client-certificate: admin.crt
	client-key: admin.key





		- viewing contents of kubeconfig file:
			- kubectl config view

		- changing the current context:
			- e.g.: kubectl config use-context prod-user@production




	* API groups
		- to check the version of kubeapi-server:
			- curl https://kube-master:6443/version
		- version API:
			- to check the version of K8S
		- metrics and healthz APIs:
			- to monitor the health of the cluster
		- logs API:
			- used for integrating with third-party logging applications
		
		- APIs responsible for cluster's functionality:
			- core group
				- /api
				- this is where all core functionalities exist(namespaces, pods, secrets,...)
				- listing core APIs:
					- curl http://localhost:6443 -k
			- named group
				- /apis
				- this is more organized
				- this group includes apis like /apps, /extensions, networking.k8s.io, ...
				- listing named APIs:
					- curl http://localhost:6443/apis -k | grep "name"


* Role Based Access Controls(RBAC)
	- To create a Role using a definition file(e.g.):

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]

- apiGroups: [""]
  resources: ["ConfigMap"]
  verbs: ["create"]



	- To bind an RBAC to a user using a definition file(e.g.):

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-Binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
kind: Role
name: developer
apiGroup: rbac.authorization.k8s.io


	- To view the created Roles:
		- kubectl get roles
	- To view the created Role bindings:
		- kubectl get rolebindings
	- To get the details of a role
		- kubectl describe role [role-name]
	- To get the details of a role binding
		- kubectl describe rolebinding [rolebinding-name]
	- To check if your current user has access to a resource in a particular cluster(e.g.):
		- kubectl auth can-i create deployments
		OR
		- kubectl auth can-i delete nodes
	- To check if a particular user has access to a resource in a particular cluster(e.g.):
		- kubectl auth can-i create deployments --as dev-user
		OR
		- kubectl auth can-i create pods --as dev-user



* Cluster Roles
	-  cluster roles are like roles, except they are for cluster-scoped(non-namespaced) resources
	- for example to create a cluster role to define access to nodes in a cluster, use the following definition file:


apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "get", "create", "delete"]


	- To bind/link a user to a cluster role use the following definition file:


apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-Binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
kind: ClusterRole
name: cluster-administrator
apiGroup: rbac.authorization.k8s.io




* Security Contexts
	- Security Context can be defined in POD and Container level
	- example of defining SecurityContext in a pod definition file:


apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
       - name: ubuntu
	     image: ubuntu
		 command: ["sleep", "3600"]
		 
		 securityContext:
		   runAsUser: 1000

		   capabilities:
		       add: ["MAC_ADMIN"]


* Network policy
	- example of creating a network policy:


apiVersion:networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
   podSelector:
     matchLabels:
	   role: db
policyTypes:
- Ingress
ingress:
- from:
  - podSelector:
      matchLabels:
	    name: api-pod
  ports:
  - protocol: TCP
    port: 3306
 


 	- Solutions that support Network Policies:
		- Kube-router
		- Calico
		- Romana
		- Weave-net
	
	- Solutions that Do Not Support Network Policies:
		- Flannel




** Storage
	
	* Storage in Docker
		- file system structure of Docker Storage
			- /var/lib/Docker
				- aufs
				- containers
				- image
				- volumes
		
		- Docker's layered Architecture
			- each line of instruction in Dockerfile creates a new layer 
			- each layer only stores the changes from the previous layer
			- when building the image with "docker build" command, it gets ReadOnly
			- when running a container using the created image (e.g) "docker run sample/my-custom-app", a new writeable layer (called Container Layer) will be created on top of all the image layers
		- creating a persistent volume in Docker container(e.g.)
			- docker volume create data_volume
			- docker run -v data_volume:/var/lib/mysql mysql
		- To mount any other directory/volume other than the ones located at /var/lib/docker/volumes, you can use the following format(this is called bind mounting ):
			- (e.g) docker run -v /data/mysql:/var/lib/mysql mysql
		- the new format of mounting(bind mounting):
			- (e.g.) docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql
		
		* Storage Drivers
			- Storage drivers help manage storage on images and containers
			- Docker uses Storage Drivers to enable layered architecture
			- Some of the common storage drivers:
				- AUFS
				- ZFS
				- BTRFS
				- Device Mapper
				- Overlay
				- Overlay2
			- Selecting storage driver depends on the OS
			* Note: Volumes are not handled by Storage Drivers
		
		* Volume Drivers
			- Volumes are handled by Volume Drivers Plugin
			- the Default Volume Driver Plugin is "Local"
			- The other volume drivers
				- Azure File Storage
				- Convoy
				- DigitalOcean Block Storage
				- Flocker
				- gce-docker
				- GlusterFS
				- NetApp
				- RexRay
				- Portworx
				- Vmware vSphere Storage
		
		
	* Container Runtime Interface(CRI)
		- It's the standard that defines how an orchestration solution like K8S would communicate with Container Runtimes(e.g. Docker)
		- In the future, if any new Container Runtime is developed, They can simply follow the CRI standards and that new Container Runtime would work with K8S easily
	
	* Container Network Interface(CNI)
		- To extend support for different Networking Solutions(e.g. Weaveworks, flannel, cilium), the Container Network Interface is introduced

	* Container Storage Interface(CSI)
		- To support multiple storage solutions(e.g. Portworx, Amazon EBS, DELL EMS, GlusterFS), CSI was introduced

	* Volume Mounting in K8S Pods
		- example of volume mounting via a Pod Definition file:

apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - image: alpine
    name: alpine
	command: ["/bin/sh", "-c"]
	args: ["shuf -i 0-100 n 1 >> /opt/number.out"]
	volumeMounts:
	- mountPath: /opt
	  name: data-volume

  volumes:
  - name: data-volume
  hostPath:
     path: /data
	 type: Directory



	
	* Persistent Volumes(PV)
		- example of defining a PV via a definition file:

apiVersion: v1
kind: PersistentVolume
metadata:
   name: pv-vol1
spec:
  accessModes:
      - ReadWriteOnce 
  capacity:
      storage: 1Gi
  hostPath:
     path: /tmp/data


	* Persistent Volume Claims(PVC)
		- Persistent Volumes and Persistent Volume Claims are two seperate objects in the K8S namespace
		- PVs are assigned/bined based on the PVCs
		- Selectors can also be used on binding
		- To view the PVCs:
			- kubectl get persistentvolumeclaim
		- Example of PVC definition file:


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
     - ReadWriteOnce

  resources:
     requests:
	   storage: 500Mi

		- To delete PVCs:
			- kubectl delete persistentvolumeclaim myclaim

		
		- Persistent Volume Reclaim Policy:
			- Retain
			- Delete
			- Recycle


** Networking

	- Creating a Network Namespace
		- (e.g.)  ip netns add red
	
	- Listing the Namespaces
		- ip netns
	
	- Listing the interfaces on the host
		- ip link
	
	- Listing the interfaces inside a Namespace
		- (e.g.) ip netns exec red ip link
				   OR
				 ip -n red link
	- Listing the Route Table inside a Namespace
		- (e.g.) ip netns exec red route

	- Creating a link between two VMs
		- (e.g.) ip link add veth-red type veth peer name veth-blue
	- Creating a namespace for a link
		- (e.g.) ip link set veth-red netns red
	- adding "IP Address" to an interface on a specific namespace
		- (e.g.) ip -n red addr add 192.168.15.1 dev veth-red
	- bringing the interfaces up in a namespace
		- (e.g.) ip -n red link set veth-red up
	- Adding Route to a specific namespace
		- (e.g.) ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5


	* Container Runtime Interface
		- Container Runtime must create network namespace
		- Identify network the container must attach to
		- Container Runtime to invoke Network Plugin (bridge) when container is ADDed
		- Container Runtime to invoke Network Plugin (bridge) when container is DELeted
		- JSON format of the Network Configuration

	* Cluster Networking
		- K8S uses the following ports
			- ETCD Port 2379 & 2380
			- Kube-API Port 6443
			- Kubelet Port 10250
			- Kube-Scheduler Port 10251
			- Kube-Controller-Manager Port 10252
	
	* Pod Networking
		- Networking modules
			- Every POD should have an IP Address
			- Every POD should be able to communicate with every other POD in the same node
			- Every POD should be able to communicate with every other POD on the other nodes without NAT
		
		- To configure CNI:
			- in kubelet config file:
				- set "--cni-conf-dir=/etc/cni/net.d" (as an example path)
				- set "--cni-bin-dir=/etc/cni/bin" (as an example path)
				- use "./net-script.sh add <container> <namespace>" command format to run a container with network interfaces on the script
					- format of net-script.sh

	ADD)
	 # Create veth pair
	 # Attach veth pair
	 # Assign IP Address
	 # Bring Up Interface
	 ip -n <namespace> link set ......
	DEL)
	 # Delete veth pair
	 ip link del .....




	* Weaveworks
		- is one of CNI solutions 
		- Weave can be deployed as daemons on each node of the Cluster or they can be deployed as PODs in the cluster
		- Deploying weave:
			- kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
			- this will deploy weave as daemonset(one POD per each node)
			- to get the deployed pods:
				- kubectl get pods -n kube-system
	

		- IP Address Management
		 	- CNI Plugin Responsibilities
				- Must support arguments ADD/DEL/check
				- Must support parameters container id, network ns, etc.
				- Must manage IP Address assignment to PODS
				- Must return results in a specific format
			- Weave by default allocates the IP addresses in the range of 10.32.0.0/12



	* CoreDNS in Kubernetes
		- CoreDNS is deployed as a POD(in 2 PODs for redundancy) in the kube-system namespace
		- It requires a configuration file(e.g. /etc/coredns/Corefile)
		- in the kubelet configuration file, the IP and Domain of the DNS Server is specified(ClusterDNS&ClusterDomain)

	* Ingress
		- Ingress helps your users access your application using a single externally accessible URL that you can configure to route
		to different services within your cluster based on the URL POD
		- It also implements SSL security 
		- Ingress is a L7 loadbalancer built-in to the K8S cluster that can be configured using native K8S primitives just like
		any other object in the K8S
		- you should expose it to make it accessible to outside the cluster(using NodePort or Loadbalancer)
		- Steps to use Ingress:
			1) Deploy
				- deploying a supported solution(e.g. NGINX, HAPROXY, TRAEFIK, Contour, Istio) which is called an "INGRESS CONTROLLER"
				- K8S Cluster doesn't come with INGRESS CONTROLLER by default
				- example of defining an NGINX INGRESS CONTROLLER using a YAML definition file:


apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas:
  selector:
    matchLabels:
	  name: nginx-ingress
  template:
    metadata:
	  labels:
	    name: nginx-ingress
	spec:
	  containers:
	    - name: nginx-ingress-controller
		  image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
	  args:
	    - /nginx-ingress-controller
		- --configmap=$ (POD_NAMESPACE) /nginx-configuration

	  env:
	    - name: POD_NAME
		  valueFrom:
		    fieldRef:
			  fieldPath: metadata.name
		- name: POD_NAMESPACE
		  valueFrom:
		    fieldRef:
              fieldPath: metadata.namespace

	  ports:
	    - name: http
		  containerPort: 80
		- name: https
		  containerPort: 443



				- we then require a service to expose the Ingress to the outside. An example service definition:


apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec: 
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
	protocol: TCP
	name: http
  - port: 443
    targetPort: 443
	protocol: TCP
	name: https
  selector:
    name: nginx-ingress

 

				- we then require a service-account with a set of specific set of permissions with correct Roles and RoleBindings


apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  


			2) Configure
				- use a set of rules which are called "INGRESS RESOURCES" to configure INGRESS(using definition files)
				- defining rules(example)
			 

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
	  - path: /wear
	    backend:
		  serviceName: wear-service
		  servicePort: 80
	  - path: /watch
	    backend:
		  serviceName: watch-service
		  servicePort: 80



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HELM ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- It's called a package manager for K8S
- commands(e.g.):
	- helm install wordpress
	- helm upgrade wordpress
	- helm rollback wordpress
	- helm uninstall wordpress
	- helm install my-site bitnami/wordpress
		- here, "my-site" is release-name

- Installing Helm(Ubuntu)
		#sudo apt-get install curl gpg apt-transport-https --yes
		#curl -fsSL https://packages.buildkite.com/helm-linux/helm-debian/gpgkey | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
		#echo "deb [signed-by=/usr/share/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any/ any main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
		#sudo apt-get update
		#sudo apt-get install helm

- Helm Components
	- Chart: Collection of files and they contain all the instructions that helm needs to know to be able to create the collection of objects that you need in K8S Cluster
	- Release: 
		- When a chart applies to a cluster, a Release is created. A single installation of an Application using a helm chart
		- Within each release, you have multiple revisions.
		- Each revision is like a snapshot of the application
	- Metadata


- Helm Repositories
	- Appscode
	- TrueCharts
	- Bitnami
	- Community Operators
	- ArtifactHub.io
		- All of the repositories' charts are listed here

- Helm Charts
	- Directory Structure
		- Templates
		- values.yaml (Configuration Values)
		- Chart.yaml  (Chart Information)
		- LICENSE     (Chart License)
		- README.md   (Readme file)
		- Charts	  (Dependency Charts)

	- Searching charts
		- (e.g.) helm search wordpress
		- (e.g.) helm search hub wordpress
			- this will search for "wordpress" charts on ArtifactHub.io Chart Repository

	- Installing a chart
		- first add the repo
			- e.g.: helm repo add bitnami https://charts.bitnami.com/bitnami
		- Then install the chart
			- e.g.: helm install my-release bitnami/wordpress
	- Listing existing charts
		- helm list
	- Uninstalling an existing chart
		- e.g. helm uninstall my-release

	- Customizing Chart Parameters
		- using the set option when installing the chart
			- e.g.: helm install --set wordpressBlogName="Helm Tutorials" my-release bitname/wordpress
		- using a custom values.yaml file
			- e.g. helm install --values custom-values.yaml my-release bitname/wordpress
			- custom-values.yaml:
				wordpressBlogName: Helm Tutorials
				wordpressEmail: john@example.com
		- changin the built-in(original) values.yaml file
			- first pull the chart
				- e.g.: helm pull --ntar bitnami/wordpress
				- change the values in values.yaml file and save it
				- install the chart using:
					- helm install my-release ./wordpress
	- Lifecycle Management with Helm
		- get the revisions
			- e.g. helm history my-release
		- rollback to a specific revision
			- e.g. helm rollback my-release 1


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kustomize ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Why to use it?
	- It's introduced to solve the problem of defining the customized K8S configs for different environments
	- We want a way to reuse our K8S configs and only modify what needs to be changed in a per-environment basis

- base
	- the config that's going to be identical across all your environments 
	- any configs or resources that you know to be the same across all your environments, you put on your base config
	- it also presents default values
	- base example:

		apiVersion:apps/v1
		kind: Deployment
		metadata:
		  name: nginx-deployment
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      component: nginx
		   template:
		     metadata:
		       labels:
		         component: nginx
		     spec:
		       containers:
		         - name: nginx
		           image: nginx


- overlays
	- the overlays allow us to customize the behaviour on a per-environment-basis
	- so we create an overlay for each one of our environments
	- overlay examples(for different environments):

		- overlays/dev
			spec:
			  replicas:1

		- overlays/stg
			spec:
			  replicas:2

		- overlays/prod
			spec:
			  replicas:5

- Forder structure
	- When using Kustomize, your folder structure will look like this(e.g.):

k8s/
	base/
		kustomization.yaml
		nginx-depl.yaml
		service.yaml
		redis-depl.yaml
	overlays/
		dev/
			kustomization.yaml
			config-map.yaml
		stg/
			kustomization.yaml
			config-map.yaml
		prod/
			kustomization.yaml
			config-map.yaml

- Kustomize comes built-in with kubectl so no other packages need to be installed
	- May still want to install the kustomize cli to get the latest version. kubectl doesn't come with the latest version
- Does not require learning how to use any complex & hard to read templating systems(like helm)
- Every artifact the Kustomize uses is in plain YAML and can be validated and processed as such

- Kustomize vs Helm
	- Helm makes use of go templates to allow assigning variables to properties
	- Helm is more than just a tool to customize configurations on a per environment basis. Helm is also a package manager for your App
	- Helm provides extra features like conditionals, loops, functions, and hooks
	- Helm templates are not valid YAML as they use go templating syntax
		- Complex templates become hard to read

- Start using Kustomize
	- first install it
	- then you must create a file named "kustomization.yaml"
		- This file should contain the list of K8S resources that should be managed by kustomize
		- This file should also contain all of the customizations that we want to apply
	- So for example we have these files as resources(under k8s directory) to be customized along with kustomization.yaml
		- nginx-depl.yml
		- nginx-service.yml
		- kustomization.yaml
	- kustomization.yaml looks like this:
		#kubernetes resources to be managed by kustomize
		resources:
		    nginx-deployment.yaml
		    nginx-service.yaml
		
		#customizations that need to be made
		commonLabels:
		  company: KodeKloud
	- now we run the following command to combine all the manifests/resources and apply the defined transformations:
		- kustomize build k8s/
	- The "kustomize build" command does not apply/deploy the kubernetes resources to a cluster
		- The output needs to be redirected to the kubectl apply command

- Kustomize Output
	- The command to run Kustomize, build all the configs and they apply those configs
		- (e.g.) kustomize build k8s/ | kubectl apply -
	- The other way to apply the kustomize is:
		- kubectl apply -k k8s/	 
	- To delete the Kustomize:
		- kustomize build k8s/ | kubectl delete -f -
		or
		- kubectl delete -k k8s/

- Multiple Directories
	- When our configs are in multiple directories, kustomize can be helpful
	- suppose here is the directory structure:
		k8s/
			kustomization.yaml
			api/
				api-depl.yaml
				api-service.yaml
			db/
				api-depl.yaml
				api-service.yaml

	- and here is the contents of kustomization.yaml:
		apiVersion: kustomize.config.k8s.io/v1beta1
		kind: Kustomization
		
		# kubernetes resources to be managed by kustomize
		resources:
		  - api/api-depl.yaml
		  - api/api-service.yaml
		  - db/db-depl.yaml
		  - db/db-service.yaml
	- so the only command we need to customize and apply the configs is:
		- kustomize build k8s/ | kubectl apply -f -
		or
		- kubectl apply -k k8s/

	- however, when the number of directories and configs get more, we can add a seperate kustomization.yaml 
	  in each directory along with another kustomization.yaml file on root directory:
		k8s/
			kustomization.yaml
			api/
				kustomization.yaml
				api-depl.yaml
				api-service.yaml
			db/
				kustomization.yaml
				db-depl.yaml
				db-service.yaml
			cache/
				kustomization.yaml
				cache-depl.yaml
				cache-service.yaml
			kafka/
				kustomization.yaml
				kafka-depl.yaml
				kafka-service.yaml
		- then the kustomization.yaml in one of the subdirectories looks like this:
				- k8s/db/kustomization.yaml
					resources:
					  - db-depl.yaml
					  - db-service.yaml
		- and the kustomization.yaml in root directory looks like this:
				- k8s/kustomization.yaml
					resources:
					  - api/
					  - db/
                      - cache/
                      - kafka/
		- To run these configs, again:
			- kustomize build k8s/ | kubectl apply -f -
			or
			- kubectl apply -k k8s/
		- this way we can keep the kustomization.yaml in root directory as clean as possible





============================================================================================ KUBERNETES In Action Book - Marko Luksa ======================================================================================================
**Pods: Running Containers in Kubernetes

	* Forwarding a Local Network Port to a Port in the POD
		- kubectl port-forward [pod-name] [local-port]:[pod-port]


** Replication and other Controllers: Deploying Managed Pods

	* Keeping PODs Healthy
		* Liveness Probes
			- Kubernetes can check if a container is still alive through "liveness probes"
			- Kubernetes can probe a container using one of the three mechanisms
				- HTTP GET 
				- TCP Socket
				- Exec Probe
			- Adding a liveness probe to a pod(e.g.):

apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080


			- 3 Parts of a Replication Controller:
				- Label Selector
				- Replica Count
				- POD Template

- Common Transformers

