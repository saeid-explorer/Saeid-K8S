============================================================================================ KUBERNETES CKA Course - UDEMY_ MUMSHAD ======================================================================================================

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Cluster Architecture~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Note: The purpose of Kubernetes is to host your applications in the form of containers in an automated fasion so that you can deploy as many instances of your application and easily enable communication between different services within your application.

	- Types of Nodes and their components in Kubernetes
		
		- Master Nodes: Manage, Plan, Schedule, Monitor Nodes
			
			- Etcd Cluster
				- Etcd is a database that stores information in a key-value format

			- Kube-Scheduler
				- A scheduler identifies the right node to place a container on based on the containers reousrce requirements, the worker nodes capacity or other constraints

			- Controller Manager
				- Node-Controller
					- Takes care of nodes. Responsible for onboarding new nodes to the cluster handling situations where nodes become unavailable
				- Replication-Controller
					- Ensures that the disired number of containers are running at all times in your replication group 
				
			- Kube-api server
				- It's the primary management component of kubernetes
				- It's responsible for orchestrating all operations within the cluster
				- It Exposes the Kubernetes API which is used for external users to perform management operations on the cluster 


		- Worker Nodes: Host Application as Containers
				
			- Container Runtime Engine
				- Softwares that can run containers
				- Popular ones are Docker, Containerd, Rocket
				- Container Runtime Engine must be installed on all the nodes in the cluster(including the Master nodes)

			- Kubelet
				- A kubelet is an agent that runs on each node in a cluster
				- It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required
				- Kube-api server priodically fetches status reports from the kubelet to monitor the state of nodes and containers on them

			- kube-proxy
				- Communication between worker nodes are enable by this on the worker node
				- Ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ETCD ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- ETCD is a distributed reliable key-value store that is simple, secure & fast
	- Installing
		1) Download Binaries
		2) Extract
			- tar xzvf <filename.tar.gz>
		3) Run ETCD Service
			- ./etcd
	- ETCD service listens on port 2379 by default
	- The default client that comes with ETCD is ETCD control client(etcdctl)
		- You can use it to store and retrieve key-value pairs
			- e.g.:  ./etcdctl set key1 value1
			- e.g.:  ./etcdctl get key1
		- getting help
			- ./etcdctl
	- ETC includes the following data:
		- Nodes
		- PODs
		- Configs
		- Secrets
		- Accounts
		- Roles
		- Bindings
		- Others
		- and Every Change on the cluster

* Note: The practice test environments are deployed using the kubeadm tool 

	- Setup ETCD when you deploy a cluster from scratch
		- first you download the binaries of ETCD
			- e.g.:
				- wget -q --https-only "https://github.com/coreos/etcd/releases/download/v3.5.13/etcd-v3.5.13-linux-amd64.tar.gz"
		* Note: --advertise-client-url: it's the address on which ETCD listens{Server_IP:2379}

	- Setup ETC when you deoploy a cluster using kubeadm tool
		- if you setup your cluster using kubeadm, then kubeadm deploys the ETCD server for you as a POD in the kube-system namespace
		- kubectl get pods -n kube-system
		- To list all keys stored on ETCD by Kubernetes:
			- kubectl exec etcd-master -n kube-system etcdctl get / --prefix --keys-only

	- ETCD stores data in a registry and under that there are various Kubernets constructs such as nodes, pods, replicasets, etc.

	- ETCD in HA Environment
		- in a multi-master Kubernetes Environment we should make sure the different ETCD services know about each other
			- --initial-cluster option in etcd.service defines ETCDs on a cluster


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-API ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubectl command in fact reaches to kube-api server (e.g. kubectl get nodes )
	- kubeapi first authenticates the request and validates it
	- it then retrieves the data from ETCD Cluster and responds back with the requested information
	- Actions as a result of a change request(e.g. creating a POD) in the cluster
		- Authenticate User
		- Validate Request
		- Retrieve data
		- Update ETCD
		- Scheduler
		- Kubelet

	- viewing api-server(kube-api) - when you deploy cluster using kubeadm
		- kubectl get pods -n kube-system 
		OR
		- cat /etc/kubernetes/manifests/kube-apiserver.yaml

	- viewing api-server(kube-api) - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-apiserver.service
		- ps -aux | grep kube-apiserver



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube Controller Manager ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- a Controller is a process that continuously monitors the state of various components within the system
	- Node Controller
		- checks the status of the nodes every 5 seconds
		- it waits for 40s before marking a node as unreachable
		- After a node is marked unreachable, it gives it five minutes to come back up, if it doesn't, it removes the PODs assigned to that node
	- Replication Controller
		- is responsible for monitoring the status of replica sets and ensuring the desired number of PODs are available at all times within the set
		- if a POD dies, it creates another one 
	- There are various other controllers
	- Installing kube-controller-manager
		- download the binaries
		- run it
			- kube-controller-manager.service
	- View kube-controller-manager - in a kubeadm setup
		- kubectl get pods -n kube-system
		- cat /etc/kubernetes/manifests/kube-controller-manager.yaml
	- View kube-controller-manager - in a non-kubeadm setup
		- cat /etc/systemd/system/kube-controller-manager.service
		- ps -aux | grep kube-controller-manager


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-Scheduler ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it decides on which Node the PODs to be placed
	- it decides based on some criteria such as resource requirements of PODS
	- here is the procedure of kube-scheduler
		1) Filter Nodes
			- ignore the nodes which don't satisfy the resource limits of the PODs
		2) Rank Nodes
			- it uses a priority function to assign a score to the nodes on a scale of 0 to 10
	- Installing kube-scheduler
		- Download the binaries
		- Extract it
		- Run it as a service
			- kube-scheduler.service
	- View kube-scheduler options - in a kubeadm setup
		- cat /etc/kubernetes/manifests/kube-scheduler.yaml
		- ps -aux | grep kube-scheduler


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kubelet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- kubelet in the kubernetes worker node, registers the node with the kubernetes cluster
	- it loads or unloads containers on the ship as instructed by the scheduler on the master
	- send back reports on the regular intervals to the master
	- Installing kubelet:
		- download the binaries
		- run it
			- kubelet.service
		* Note: kubeadm does not automatically deploy the kubelet
	- you must always manually install the kubelet on your worker nodes 
	- View kubelet options
		- ps -aux | grep kubelet


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Kube-proxy ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- it's a process that runs on each node in the kubernetes cluster
	- its job is to look for new services and every time a new service is created, it creates the appropriate rules on each node to forward traffic to those services to the backend PODs
	- One way it works is using IPTABLES rules
	- Installing kube-proxy
		- download the binaries
		- extract it
		- run it
			- kube-proxy.service
	- View kube-proxy - in kubeadm setup
		- kubectl get pods -n kube-system
		- kubectl get daemonset -n kube-system

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- The containers are encapsulated into a Kubernetes object known as POD
	- A POD is a single instance of an application in the form of a Container
	- A POD is the smallest object that you can create in Kubernetes
	- To scale our Application, we don't create new containers to the existing PODs, we create new PODs
	- deploying pods:
		- e.g.: kubectl run nginx --image nginx
	- Viewing the pods in our Kubernetes cluster
		- kubectl get pods

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PODs with YAML ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	* Example of a YAML file for creating pod(s)


apiVersion: v1

kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp

spec:
  containers:
    - name: nginx-container
      image: nginx

    - name: backend-container
      image: redis



    - To create PODs using YAML file
    	- kubectl create -f <file_name.yml>



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Replication Controller ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	- provides
		- HA
		- Load Balancing & Scaling
	- Replica Set is the new recommended way to setup replication. Replication Controller was the old way to do that

	* Example of defining replication controller using a YAML file (first metadata section relates to Replication Controller, the second metadata section under Template is related to POD definition)


apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3


  	- To create the replication controller using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To view the list of replication controllers
  		- kubectl get replicationcontroller



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ReplicaSet ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	* Example of defining replication controller using a YAML file (Selector section in this file is required)


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 


  	- To create the replicaSet using the YAML file
  		- kubectl create -f <file_name.yml>

  	- To delete the replicaSet
  		- kubectl delete replicaset <ReplicaSet_Name>

  	- To view the list of replication controllers
  		- kubectl get replicaset

  	- Labels and Selectors
  		- These are mechanisms used by replicaSet to know which PODs to monitor, so that it can bring up new PODs in case any of them failed

  	- Scaling ReplicaSet
  		- There are some ways
  			1) update the number of "replicas: <#>" in .yml difinition file
  			   Then use "kubectl replace -f <file_name.yml>" command to update the ReplicaSet

  			2) Use "kubectl scale --replicas=<#> -f <file_name.yml>" to update the ReplicaSet

  			3) Use "kubectl scale --replicas=<#> replicaset <ReplicaSet_Name>"
  				- e.g.: kubectl scale --replicas=6 replicaset myapp-replicaset


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Deployments ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	- Deplolyment provides us with the capability to upgrade the underlying instances seamlessly using
		- rolling updates
		- undo changes
		- pause changes
		- resume changes

	- Deployment Definition file(.yml)
		-  the definition of Deployment is exactly as the definition of ReplicaSet, Except the "kind:" is going to be Deployment

	* Note: The Deployment automatically creates ReplicaSet
 
	* Example of defining Deployment using a YAML file


apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-replicaset
  labels:
     app: myapp
     type: front-end

spec:
  template:

    metadata:
      name: myapp-pod
      labels:
        app:myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3
  selector:
    metchLabels:
      type: front-end 




  	- To create the Deployment using the YAML file
  		- kubectl create -f <file_name.yml>


  	- To view the list of Deployments
  		- kubectl get deployments


	* Note: To see all the created objects
		- kubectl get all



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Namespaces ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	- There are initially 3 Namespaces created inside Kubernetes
		1) Default
			- it's created automatically in Kubernetes when the Cluster is first set up
			- by default we are in this namespace
		2) kube-system
			- Kubernetes creates a set of Pods and services for its interanl purpose such as networking, DNS, etc
			- To isolate these Pods from the user and prevent from accidentally deleting or modifying these services, Kubernets creates them under this namespace created at cluster startup
		3) kube-public
			- This is where resources that should be made available to all users are created

	* Note: you can create your own namespaces 
	- Each of the namespaces has its own set of policies. For example you can set a certain quota of resources to each namespace
	- The resources inside a namespace can refer to each other simply by their names

	- for refering to the services inside another nameservice, we should append the name of the namespace to the name of the service
		- {service/Pod_name}.{namespace}.svc.cluster.local
		- cluster.local is the default domain name of the Kubernetes cluster
		- SVC is the subdomain for service
		- e.g.: mysql.connect("db-service.dev.svc.cluster.local")

	- To list PODs in a specific namespace
		- kubectl get pods --namespace={namespace_name}
		- e.g.:  kubectl get pods --namespace=kube-system

	- To create a POD using a YAML file in a specific namespace
		- kubectl create -f {Pod-definition-file.yml} --namespace={namespace_name}
		- e.g.:  kubectl create -f pod-definition.yml --namespace=dev
		- you can also place "namespace:" directive in the YAML file under "metadata:" section to make sure your resources are always created inside a specifc namespace


	- To create a namespace using a namespace definition(YAML) file
		- kubectl create -f {namespace-file.yml}

		* Example of a namespace .yml definition file

apiVersion: v1
kind: Namespace
metadata:
    name: dev



	- Another way to create namespace
		- kubectl create namespace {namespace_name}

		
	- To switch the namespace permanently
		- kubectl config set-context $(kubectl config current-context) --namespace={namespace_name}
		- e.g.:  kubectl config set-context $(kubectl config current-context) --namespace=dev

	- To view PODs in all namespaces
		- kubectl get pods --all namespaces

	* Note: Contexts are used to manage multiple clusters in multiple environments from the same management system


	* Example of creating a resource quota for a namespace


apiVersion: v1
kind: ResourceQuota
metadata:
    name: compute-quota
    namespace: dev

spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi




	* Note: to create a template yaml file from creating a pod or any other object
		- e.g.: kubectl run redis --image=redis --restart=Never --dry-run -o yaml > pod.yaml
		- The above command will create a YAML file which we can edit and then create a pod by the command "kubectl apply -f pod.yaml"


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Services ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Kubernetes Services enable communication between various components within and outside of the application
- Services help us connect applications together with other applications or users


** Service Types
	1) NodePort
		- One of the use cases of Services is to listen to a port on the Node and forward requests on that port to a port on the POD running the application

	2) ClusterIP
		- The service creates a virtual IP inside the cluster to enable communication between different services such as a set of front-end services to a set of backend services

	3) LoadBalancer
		- it provides a load balancer for our application in supported cloud providers


** Service - NodePort
	- TargetPort: Pod's Port
	- Port: Service's Port
	- NodePort: Node's Port - can only be in a valid range which by default is 30000-32767
	- If the port is not set manually, it would be set similar to TargetPort
	- If the NodePort is not set manually, a random Port in the valid range would be automatically allocated
	- selector section of the YAML file selects the PODs that will be used for NodePort service

	* Example of defining a NodePort Service via YAML file

apiVersion: v1
kind: Service
metadata:
    name: myapp-service
    
spec:
    type: NodePort
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: front-end

	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



** Service - Cluster IP
	- A Kubernetes Service that can help us group different sets of PODs together and provide a single interface to access the PODs in a group
	- The requests are forwarded to one of the PODs under the service randomly
	- This enables us to easily and effectively deploy a microservices-based application on Kubernetes cluster
	- Each Service gets an IP and Name assigned to it inside the cluster
	- TargetPort in YAML file is the port where the application/POD is exposed
	- Port in YAML file is where the service is exposed
	- To link the service to a set of PODs we use Selector

	* Example of defining a Cluster IP Service via YAML file


apiVersion: v1
kind: Service
metadata:
    name: back-end

spec:
    type: ClusterIP
    ports:
     - targetPort: 80
       port: 80
       nodePort: 30008
    selector:
	app: myapp
	type: back-end


	- To create the Service using YAML file
		- kubectl create -f <service-difinition.yml>

	- To get the list of Services
		- kubectl get services



* Note: to expose port of a Service:
	- e.g.: kubectl expose pod redis --name=redis-service --port=6379 --target-port=6379




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Initialize Kubernetes Master Node with Kubeadm ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 	- In order to initialize a kubernetes cluster with Kubeadm
		- kubeadm  init --pod-network-cidr <cidr>
		- e.g.:
			kubeadm init --pod-network-cidr 10.88.0.0/16

	* Note: If you have already run the kubeadm initialization command and confronted an error, do the following to remove the configs before running the initialization again
		- rm .kube/config

	* validation commands
		- kubectl version
		- kubectl get all --namespace kube-system
		- kubectl get all --namespace default

	* Important common error
		- The connection to the server <Host IP>:6443 was refused - did you specify the right host or port?
		- To solve this error, use the following commands sequentially:
			- sudo -i
			- swapoff -a
			- exit
			- strace -eopenat kubectl version
			
	* Installing CNI(Container Network Interface)
		- You must deploy a Container Network Interface (CNI) based Pod network add-on so that your Pods can communicate with each other 
		- Cluster DNS (CoreDNS) will not start up before a network is installed
		- Calico is an open-source CNI
			- Calico has network policy capability
			- to install Calico, go to Calico github page -> getting started section

