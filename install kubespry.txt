- apt update
- apt upgrade -y
- vi /etc/ssh/sshd_config
- systemctl restart ssh
*********
- netstat -rn
- vi /etc/netplan/00-installer-config.yaml
network:
 version: 2
   # renderer: NetworkManager
 ethernets:
   ens33:
     dhcp4: no
     addresses: [192.168.89.141/24]
     #routes:
      #- to: default
      #  via: 192.168.239.2
     nameservers:
         addresses: [8.8.8.8,8.8.8.4]
*********
hostnamectl set-hostname ansible
hostnamectl set-hostname master-1
hostnamectl set-hostname master-2
hostnamectl set-hostname master-3
hostnamectl set-hostname worker-1
hostnamectl set-hostname worker-2


vi /etc/hosts
192.168.80.141 ansible
192.168.89.143 node1
192.168.89.144 node2
192.168.89.145 node3
192.168.89.140 node4
192.168.239.138 node5

******************
ssh-keygen
ssh-copy-id 192.168.89.137
ssh-copy-id 192.168.89.137
ssh-copy-id 192.168.89.139
ssh-copy-id 192.168.89.140
ssh-copy-id 192.168.239.138

***********


- apt install git python3 python3-pip -y
- git clone https://github.com/kubernetes-incubator/kubespray.git
- cd kubespray
- pip install -r requirements.txt
- ansible --version
- cp -rfp inventory/sample inventory/mycluster
- declare -a IPS=(192.168.89.137 192.168.89.138 192.168.89.139 192.168.89.140)
- CONFIG_FILE=inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}
**************
- vi inventory/mycluster/hosts.yaml

all:
  hosts:
    master-1:
      ansible_host: 192.168.239.134
      ip: 192.168.239.134
      access_ip: 192.168.239.134
    master-2:
      ansible_host: 192.168.239.135
      ip: 192.168.239.135
      access_ip: 192.168.239.135
    master-3:
      ansible_host: 192.168.239.136
      ip: 192.168.239.136
      access_ip: 192.168.239.136
    worker-1:
      ansible_host: 192.168.239.138
      ip: 192.168.239.138
      access_ip: 192.168.239.138
    worker-2:
      ansible_host: 192.168.239.137
      ip: 192.168.239.137
      access_ip: 192.168.239.137
  children:
    kube_control_plane:
      hosts:
        master-1:
        master-2:
        master-3:
    kube_node:
      hosts:
        master-1:
        master-2:
        master-3:
        worker-1:
        worker-2:
    etcd:
      hosts:
        master-1:
        master-2:
        master-3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
**********************
- vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
- vi inventory/mycluster/group_vars/k8s_cluster/addons.yml
-----------
dashboard_enabled: true
ingress_nginx_enabled: true
ingress_nginx_host_network: true
-----------

- ansible all -i inventory/mycluster/hosts.yaml -m shell -a "sudo systemctl stop ufw && sudo systemctl disable ufw"
- ansible all -i inventory/mycluster/hosts.yaml -m shell -a "echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf"
- ansible all -i inventory/mycluster/hosts.yaml -m shell -a "sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab && sudo swapoff -a"

*************************
in all node:

- vi /etc/resolv.conf
-----------
nameserver 178.22.122.100
nameserver 185.51.200.2
-------------------

**********************

Start Kubernetes deployment:

- ansible-playbook -i inventory/mycluster/hosts.yaml --become --become-user=root cluster.yml

for reset kuber:

- ansible-playbook -i inventory/mycluster/hosts.yaml reset.yml --become --become-user=root




